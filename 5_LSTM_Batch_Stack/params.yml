run:
  temperature: 1.0
  prime_input: "My lord"
  predict_len: 250

model:
  embed_size: 64
  hidden_size: 128
  num_layers: 3

data:
  file_path: "data/shakespeare.txt"
  portion_size: 50
  iters_per_epoch: 10000
  batch_size: 128

experiment:
  lr: 0.005

trainer:
  gpus: [1]
  max_time: "00:00:05:00" # DD:HH:MM:SS
  monitor: "train_loss"
  mode: "min"

app:
  name: "5_LSTM_Batch_Stack"
  logs_dir: "logs"
  manual_seed: 1234
